{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime(2020, 2, 6)\n",
    "end_date = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0) \n",
    "\n",
    "delta = timedelta(days=1)\n",
    "current_date = start_date\n",
    "output_file = f'output_{datetime.now().isoformat()}.txt'\n",
    "while current_date <= end_date:\n",
    "    a_name = f\"events-{str(current_date.date())}.jsonl\"\n",
    "    archive_url = f\"https://archive.analytics.mybinder.org/{a_name}\"\n",
    "    domain = 'notebooks.gesis.org'\n",
    "    from_dt = current_date.isoformat()\n",
    "    to_dt = (current_date+delta-timedelta(seconds=1)).isoformat()\n",
    "    api_url = f'https://{domain}/gallery/api/v1.0/launches/{from_dt}/{to_dt}'\n",
    "\n",
    "    print(a_name, from_dt, to_dt)\n",
    "    print(archive_url)\n",
    "    print(api_url)\n",
    "    \n",
    "    # first read events from archive\n",
    "    df = pd.read_json(archive_url, lines=True)\n",
    "    # handle exections in events archive\n",
    "    # events before 12.06.2019 has no origin value\n",
    "    if 'origin' not in df.columns:\n",
    "        df[\"origin\"] = \"mybinder.org\"\n",
    "    # events-2019-06-12.jsonl has mixed rows: with and without origin value\n",
    "    if a_name == \"events-2019-06-12.jsonl\":\n",
    "        df['origin'].fillna('mybinder.org', inplace=True)\n",
    "    # in some archives Gist launches have wrong provider (GitHub)\n",
    "    elif a_name == \"events-2018-11-25.jsonl\":\n",
    "        df.loc[df['spec'] == \"https%3A%2F%2Fgist.github.com%2Fjakevdp/256c3ad937af9ec7d4c65a29e5b6d454\",\n",
    "                  \"provider\"] = \"Gist\"\n",
    "        df.loc[df['spec'] == \"https%3A%2F%2Fgist.github.com%2Fjakevdp/256c3ad937af9ec7d4c65a29e5b6d454\",\n",
    "                  \"spec\"] = \"jakevdp/256c3ad937af9ec7d4c65a29e5b6d454\"\n",
    "    elif a_name == \"events-2019-01-28.jsonl\":\n",
    "        df.loc[df['spec'] == \"loicmarie/ade5ea460444ea0ff72d5c94daa14500\",\n",
    "                  \"provider\"] = \"Gist\"\n",
    "    elif a_name == \"events-2019-02-22.jsonl\":\n",
    "        df.loc[df['spec'] == \"minrk/6d61e5edfa4d2947b0ee8c1be8e79154\",\n",
    "                  \"provider\"] = \"Gist\"\n",
    "    \n",
    "    # filter out gesis launches, because archive is updated periodically, \n",
    "    # but binder gallery is updated instantly for gesis launches\n",
    "    df = df.loc[df['origin'] != domain]\n",
    "    df = df.loc[df['origin'] != \"gesis.mybinder.org\"]\n",
    "    \n",
    "    # now read launch events from binder gallery api\n",
    "    launches = []\n",
    "    # because of pagination the api gives 100 results per page \n",
    "    # so for analysis you have to take data in all pages\n",
    "    next_page = 0\n",
    "    while next_page is not None:\n",
    "        api_query_url = api_url\n",
    "        if next_page:\n",
    "            api_query_url = api_url + str('?page=') + str(next_page)\n",
    "        r = requests.get(api_query_url)\n",
    "        response = r.json()\n",
    "        # check the limit of queries per second/minute,\n",
    "        message = response.get(\"message\", \"\")\n",
    "        if message not in [\"2 per 1 second\", \"100 per 1 minute\"]:\n",
    "            launches.extend(response['launches'])\n",
    "            next_page = response['next_page']\n",
    "        else:\n",
    "            sleep(1)\n",
    "    data = pd.DataFrame(launches)\n",
    "    # filter out gesis launches, because archive is updated periodically, \n",
    "    # but binder gallery is updated instantly for gesis launches\n",
    "    df2 = data.loc[data['origin'] != \"\"]\n",
    "    df2 = df2.loc[data['origin'] != domain]\n",
    "    df2 = df2.loc[data['origin'] != \"gesis.mybinder.org\"]\n",
    "\n",
    "    # convert string timestamp to pandas timestamp\n",
    "    df2['timestamp'] = pd.to_datetime(df2['timestamp'])\n",
    "    \n",
    "    # do the comparison\n",
    "    a = df.drop(columns=['schema', 'status']).groupby([\"origin\", \"provider\", \"spec\", \"version\"]).count().sort_values(\"timestamp\", ascending=False)\n",
    "    b = df2.drop(columns=['schema', 'status']).groupby([\"origin\", \"provider\", \"spec\", \"version\"]).count().sort_values(\"timestamp\", ascending=False)\n",
    "    is_equal = a.equals(b)\n",
    "    if len(df) != len(df2) or not is_equal:\n",
    "        with open(\"error_\"+output_file, 'a') as f:\n",
    "            f.write(f\"{current_date.isoformat()}, archives len: {len(df)}, \"\n",
    "                    f\"gallery api len: {len(df2)}, diff: {len(df)-len(df2)}, is_equal: {is_equal}\\n\")\n",
    "        print(\"-------> error\")\n",
    "        #print(df.origin.unique())\n",
    "        #print(df2.origin.unique())\n",
    "        #print(set(df.origin.unique())-set(df2.origin.unique()))\n",
    "    else:\n",
    "        with open(\"success_\"+output_file, 'a') as f:\n",
    "            f.write(f\"{current_date.isoformat()}\\n\")\n",
    "            \n",
    "    current_date += delta\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
